# From-Markov-Chains-to-Self-Attention-A-Study-of-Long-Range-Sequence-Dependencies
This project experimentally shows why first-order Markov models fail on long-range dependencies and how Transformers overcome this limitation using self-attention
